## simhash算法

### 问题背景

如果某一天，面试官问你如何设计一个比较两篇文章相似性的算法？可能你会回答几个比较传统的思路：
 - 一种方案是先将两篇文章进行分词，得到一系列特征向量，进而转化为特征向量距离的度量（比如常见的欧氏距离、海明距离或者夹角余弦等等），最终通过距离的大小来判断两篇文章的相似度。
 - 另外一种方案是传统hash，我们考虑采用为每一个web文档通过hash的方式生成一个指纹（finger print）。

采取第一种方法，比较两篇文章的相似性还好，但如果是海量数据呢，有着数以百万甚至亿万的网页，要求你计算这些网页的相似度呢？你还会去计算任意两个网页的距离或夹角余弦么？这时，你会开始犯愁了。

而至于第二种方案中所说的传统加密方式md5，其设计的目的是为了让整个分布尽可能地均匀，但输入内容一旦出现轻微变化，hash值就会发生很大的变化。

举个例子，我们假设有以下三段文本： 

- the cat sat on the mat
- the cat sat on a mat
- we all scream for ice cream

使用传统hash可能会产生如下的结果：
 - irb(main):006:0> p1 = 'the cat sat on the mat' 
- irb(main):007:0> p1.hash => 415542861 
 - irb(main):005:0> p2 = 'the cat sat on a mat' 
- irb(main):007:0> p2.hash  => 668720516 
 - irb(main):007:0> p3 = 'we all scream for ice cream' 
- irb(main):007:0> p3.hash => 767429688 "

而我们理想当中的哈希函数，需要对几乎相同的输入内容，产生相同或者相近的hashcode，换言之，hash code的相似程度要能直接反映输入内容的相似程度，故md5等传统hash也无法满足我们的需求。 

### simhash出世

车到山前必有路，来自于GoogleMoses Charikar发表的一篇论文“detecting near-duplicates for web crawling”中提出了simhash算法，专门用来解决万亿级别的网页的去重任务。

simhash作为locality sensitive hash（局部敏感哈希）的一种：
 - 其主要思想是降维，将高维的特征向量映射成一个低维的特征向量，通过两个向量的Hamming Distance来确定文章是否重复或者高度近似。
- 其中，Hamming Distance，又称汉明距离，在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。也就是说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。例如：1011101 与 1001001 之间的汉明距离是 2。至于我们常说的字符串编辑距离则是一般形式的汉明距离。

如此，通过比较多个文档 simHash 值的海明距离，获取它们的相似度。

### simhash流程

simhash算法分为5个步骤：分词、hash、加权、合并、降维，具体如下所述：
 - 分词
- 给定一段语句，进行分词，得到有效的特征向量，然后为每一个特征向量设置一个5个级别（1—5）的权重，如果是给定一个文本，那么特征可以是文档中的词，其权重可以是这个词出现的次数。例如给定一段语句：“CSDN博客结构之法算法之道的作者July”，分词后为：“CSDN 博客 结构 之 法 算法 之 道 的 作者 July”，然后为每个特征向量赋予权值：CSDN(4) 博客(5) 结构(3) 之(1) 法(2) 算法(3) 之(1) 道(2) 的(1) 作者(5) July(5)，其中括号里的数字代表这个单词在整条语句中的重要程度，数字越大代表越重要。
 - hash
- 通过hash函数计算各个特征向量的hash值，hash值为01组成的n-bit签名。比如“CSDN”的hash值为100101，“博客”的hash值为“101011”。就这样，字符串就变成了一系列数字。
 - 加权
- 在hash值的基础上，给所有特征向量进行加权（1则为正，0则为负）。例如给“CSDN”的hash值“100101”加权（“CSDN”的权值为4）得到：4 -4 -4 4 -4 4，给“博客”的hash值为“101011”通过加权（“博客”的权值为5）得到：“5 -5 5 -5 5 5”，其余特征向量类似此般操作。
 - 合并
- 将上述各个特征向量的加权结果累加，变成只有一个序列串。这里拿前两个特征向量举例，例如“CSDN”的“4 -4 -4 4 -4 4”和“博客”的“5 -5 5 -5 5 5”进行累加，得到“4+5 -4+-5 -4+5 4+-5 -4+5 4+5”，得到“9 -9 1 -1 1”。
 - 降维
- 对于n-bit签名的累加结果，如果>0置1，否则置0，从而得到该语句的simhash值，最终根据不同语句simhash的海明距离就来判断相似程度。例如把上面计算出来的“9 -9 1 -1 1 9” 变成01串，根据每一位大于0 记为 1，小于0 记为 0的方法，得到的01串为：“1 0 1 0 1 1”，从而形成我们最终的simhash签名。

其流程如下图所示：
![](http://dl.iteye.com/upload/attachment/437426/baf42378-e625-35d2-9a89-471524a355d8.jpg)

### simhash的应用

 - 对每篇文档根据SimHash 算出签名后，再计算两个签名的海明距离（两个二进制异或后 1 的个数）即可。根据经验值，对 64 位的 SimHash ，海明距离在3以内的可以认为相似度比较高。 
- 注：异或时，只有在两个比较的位不同时其结果是1 ，否则结果为 0 

举个例子，上面最终计算到的“CSDN博客”的simhash签名值为“1 0 1 0 1 1”，假定我们计算出另外一个短语的签名值为“1 0 1 0 0 0”，然后根据异或规则，我们计算出这两个签名的海明距离为2，故可判定这两个短语的相似度是比较高的。

从而问题转换为：对于64位的SimHash，我们只要找海明距离在3以内的所有签名，即可计算出任意两个文本的相似度。

但关键是，如何将其扩展到海量数据的相似度的检测中去呢？譬如说如何在海量的样本库（大于1M）中查询与其海明距离在3以内的记录呢？好问题！

 - 一种方案是查找待查询文本的64位simhash code的所有3位以内变化的组合，大约需要四万多次的查询；
 - 另一种方案是预生成库中所有样本simhash code的3位变化以内的组合，大约需要占据4万多倍的原始空间。

这两种方案，要么时间复杂度高，要么空间复杂度复杂，是否有一种方案能达到时空复杂度的绝佳平衡呢？答案是肯定的：

- 我们可以把 64 位的二进制simhash签名均分成 4块，每块 16 位。根据鸽巢原理（也成抽屉原理），如果两个签名的海明距离在 3 以内，它们必有一块完全相同。如下图所示：
![](http://dl.iteye.com/upload/attachment/437559/689719df-54b7-318c-bc90-e289f84344b9.jpg)
- 然后把分成的4 块中的每一个块分别作为前 16 位来进行查找，建倒排索引。

具体如下图所示：

![](http://dl.iteye.com/upload/attachment/437586/b72b8dc2-9139-3078-ad24-b689f64fd71a.jpg)

如此，如果样本库中存有2^34（差不多10亿）的simhash签名，则每个table返回2^(34-16)=262144个候选结果，大大减少了海明距离的计算成本。
 - 假设数据是均匀分布，16 位的数据，产生的像限为2^16个，则平均每个像限分布的文档数则为2^34/2^16 = 2^(34-16)) ，四个块返回的总结果数为 4* 262144 （大概 100 万）。
- 这样，原本需要比较10亿次，经过索引后，大概只需要处理100万次。

部分内容及图片参考自：http://grunt1223.iteye.com/blog/964564 ，致谢。
